# @package _group_

common:
  fp16: false
  fp16_no_flatten_grads: true
  log_format: json
  log_interval: 100
  reset_logging: false
  suppress_crashes: false
  wandb_project: "wav2uni"
  wandb_entity: "indy-lis"

checkpoint:
  save_interval: 1000
  save_interval_updates: 1000
  keep_interval_updates: 1
  no_epoch_checkpoints: true
  best_checkpoint_metric: weighted_lm_ppl
  save_dir: .

distributed_training:
  distributed_world_size: 1

task:
  _name: unpaired_audio_text
  data: ???
  text_data: ???
  labels: phn
  sort_by_length: false
  unfiltered: false
  max_length: null
  append_eos: false
  kenlm_path: ???
  aux_target_postfix: km

dataset:
  num_workers: 0
  batch_size: 160
  skip_invalid_size_inputs_valid_test: true
  valid_subset: valid
  validate_interval: 1000
  validate_interval_updates: 1000

criterion:
  _name: model
  log_keys:
    - temp
    - code_ppl
    - grad_mmi_g
    - grad_smoothness_g
    - grad_code_pen_g
    - grad_lm_g

optimization:
  max_update: 150000
  clip_norm: 5.0
  lr: [0]

optimizer:
  _name: composite
  groups:
    generator:
      lr: [0.00005]
      lr_float: null
      optimizer:
        _name: adam
        adam_betas: [0.5,0.98]
        adam_eps: 1e-06
        weight_decay: 0
        amsgrad: false
      lr_scheduler:
        _name: fixed
        warmup_updates: 0

lr_scheduler: pass_through

model:
  _name: wav2uni

  generator_stride: 3
  generator_kernel: 9
  generator_bias: false
  generator_dropout: 0.1
  generator_batch_norm: 35
  generator_residual: true

  smoothness_weight: 0.0
  gumbel: true
  hard_gumbel: true
  code_penalty: 0.0
  temp: [ 2,0.1,0.99995 ]
  input_dim: 1024
  mmi_weight: 0.3
  target_dim: 64
  lm_ckpt: ???
  lm_window: -1

  log_gradients: false
  
  segmentation:
    type: JOIN
    mean_pool_join: false
    remove_zeros: false
